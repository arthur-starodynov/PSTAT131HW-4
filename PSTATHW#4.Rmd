---
title: "HW 4"
author: "PSTAT 131/231 Arthur Starodynov"
output:
  pdf_document:
    toc: yes
  html_document:
    toc: yes
    toc_float: yes
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(data.table)
library(tidymodels)
library(tidyverse)
library(caret)
library(ISLR)
library(datarium)
titanic.data <- read.csv('C:/Users/arthu/Dropbox/My PC (DESKTOP-9BV8I37)/Documents/pstat131/PSTAT131HW#3/titanic.csv')
set.seed(46)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

```{r}
titanic.data$pclassfac <- as.factor(titanic.data$pclass)
titanic.data$survived <- as.factor(titanic.data$survived)
```
Problem 1) 

```{r}
p <- 0.7
strats <- titanic.data$survived

rr <- split(1:length(strats), strats)
idx <- sort(as.numeric(unlist(sapply(rr, function(x) sample(x, length(x) * p)))))

train <- titanic.data[idx, ]
test <- titanic.data[-idx, ]
```

Problem 2) 

```{r}
train_fold <- vfold_cv(train, v = 10)
train_fold
```

```{r}
train_control <- trainControl(method = "cv",
                              number = 10)
kfold_train <- train(factor(survived) ~., data = train, 
               trControl = train_control,
               na.action = na.exclude)
kfold_train
```

Problem 3)
K fold cross Validation is another way to test the skill of a model in machine learning and test its accuracy. What it does is sokit the dataset in k groups, and within each of the groups it will take the group to hold the data and use the remaining groups as training data set so that it could run the model on the training set eventually running through all the other data sets to test to test is skill .Using K fold allows us to run the model on each data set making sure if there is one that does not fit well it can be unfolded and seen and then taken out. It specifies more in depth then just fitting the model on a bigger data set it allows the data set to be thinner. We would use logistic regression with the whole data set. 

Problem 4)

```{r}
simple_recipe <- recipe(survived ~ 
                      pclass+sex+age+sib_sp+parch+fare,
                        data=train) %>%
  step_impute_linear(age) %>%
  step_dummy(all_nominal_predictors(),one_hot = F) %>%
  step_interact(terms = ~starts_with("sex"):fare) %>%
  step_interact(terms = ~ age:fare)

```

```{r}
lgm_model = logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")
lgm_workflow = workflow() %>%
  add_model(lgm_model) %>%
  add_recipe(simple_recipe)
```

```{r}
library(discrim)
dis_model = discrim_linear() %>%
  set_engine("MASS") %>%
  set_mode("classification")
dis_workflow = workflow() %>%
  add_model(dis_model) %>%
  add_recipe(simple_recipe)

```

```{r}
quad_model = discrim_quad() %>%
  set_engine("MASS") %>%
  set_mode("classification")
quad_workflow = workflow() %>%
  add_model(quad_model) %>%
  add_recipe(simple_recipe)
```
we will be fitting 10 folds so 10 datasets along 3 different workflow models.

Problem 5) 

```{r}
lgm_tune <- lgm_workflow %>% 
  tune_grid(resamples = train_fold)
dis_tune <- dis_workflow %>%
  tune_grid(resamples = train_fold)
quad_tune <- quad_workflow %>%
  tune_grid(resamples = train_fold)
```
Problem 6) 
```{r}
lgm_metric <- collect_metrics(lgm_tune)
print(lgm_metric)
dis_metric <- collect_metrics(dis_tune)
print(dis_metric)
quad_metric <- collect_metrics(quad_tune)
print(quad_metric)
```
The logistic regression model was the best fitting out of the 3 it is clear due to the mean being the highest as well as having the lowest standard deviation out of all the others meaning that it would be the most accurate. 

Problem 7) 

```{r}
lgm_fit <- lgm_workflow %>%
  fit(train)
```


Problem 8) 

```{r}
predict_train_model= bind_cols(predict(lgm_fit, test),
                               test$survived)
colnames(predict_train_model) = c("GLM Predict", "True")
print(accuracy(predict_train_model, 
               truth='True', estimate="GLM Predict")$.estimate)

```
The accuracy on the GLM of the test data was lower at 78% compared to the data of the folded parts, meaning that the folded data was able to be predicted better and was more accurate at a mean/accuracy of around 82%

